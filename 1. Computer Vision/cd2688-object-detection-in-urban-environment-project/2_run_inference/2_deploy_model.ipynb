{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a195dd47",
   "metadata": {},
   "source": [
    "# Deploy model and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226e153f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sagemaker -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b3785b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker.utils import name_from_base\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import visualization_utils as viz_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbd53299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b56d7",
   "metadata": {},
   "source": [
    "Now that you have trained successfully your model, you want to look at the predictions on some sample images. To do so, you will need to find the s3 path of the exported model. You can navigate to the Training jobs section of the AWS web UI and click on the training job of interest. Scroll down and you should see something like this:\n",
    "\n",
    "![Example Artifact](../data/example_artifact.png)\n",
    "\n",
    "The model artifact path should look something like \n",
    "```s3://sagemaker-us-east-1-073338978050/tf2-object-detection-2022-10-22-21-26-37-033/output/model.tar.gz```. Use this value to update the `model_artifact` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b6ae306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Update the model artifact here. \n",
    "model_artifact = 's3://sagemaker-us-east-1-118186639883/tf2-object-detection-2024-05-28-07-46-26-337/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c71be7",
   "metadata": {},
   "source": [
    "Now we can deploy the model. Run the following cell and check that the model was correctly deployed by navigating to Inference endpoints in the web UI.\n",
    "\n",
    "![Example endpoint](../data/example_endpoints.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09c3496d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "model = TensorFlowModel(\n",
    "    name=name_from_base('tf2-object-detection'),\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    framework_version='2.8'\n",
    ")\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.g4dn.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3d9cf",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2b7b4",
   "metadata": {},
   "source": [
    "Our model is now deployed and we can query it. We are going to use the images available in `data/test_video` to run inference and generate a video. To do so, we are going to need a few tools:\n",
    "* we need to sort all the frames by index order (which corresponds to chronological order)\n",
    "* we need a function to load images into numpy array\n",
    "* we need a loop to run inference and display the results on the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2d725",
   "metadata": {},
   "source": [
    "We list the frame paths and sort them by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2656b900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames_path = sorted(glob.glob('../data/test_video/*.png'), \n",
    "                     key = lambda k: int(os.path.basename(k).split('.')[0].split('_')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4dd116",
   "metadata": {},
   "source": [
    "We create a small function to load images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56c4dc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_image(path: str) -> np.ndarray:\n",
    "    \"\"\"Read an image from the path and returns a numpy array\"\"\"\n",
    "    cv_img = cv2.imread(path,1).astype('uint8')\n",
    "    cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "    return cv_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4a04f",
   "metadata": {},
   "source": [
    "We create a mapping from id to name for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1535d202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_index = {\n",
    "                    1: {'id': 1, 'name': 'vehicle'}, \n",
    "                    2: {'id': 2, 'name': 'pedestrian'},\n",
    "                    4: {'id': 4, 'name': 'cyclist'}\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d323e30",
   "metadata": {},
   "source": [
    "This is the main loop:\n",
    "* we load images to numpy\n",
    "* we query the deployed model\n",
    "* we display the inference results on the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a491b0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/100 images.\n",
      "Processed 10/100 images.\n",
      "Processed 20/100 images.\n",
      "Processed 30/100 images.\n",
      "Processed 40/100 images.\n",
      "Processed 50/100 images.\n",
      "Processed 60/100 images.\n",
      "Processed 70/100 images.\n",
      "Processed 80/100 images.\n",
      "Processed 90/100 images.\n"
     ]
    }
   ],
   "source": [
    "def image_file_to_tensor(path):\n",
    "    cv_img = cv2.imread(path,1).astype('uint8')\n",
    "    cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "    return cv_img\n",
    "\n",
    "images = []\n",
    "for idx, path in enumerate(frames_path):\n",
    "    if idx % 10 == 0:\n",
    "        print(f'Processed {idx}/{len(frames_path)} images.')\n",
    "        \n",
    "    # load image\n",
    "    img = image_file_to_tensor(path)\n",
    "    inputs = {'instances': [img.tolist()]}\n",
    "    \n",
    "    # run inference and extract results\n",
    "    detections = predictor.predict(inputs)['predictions'][0]\n",
    "    detection_boxes = np.array(detections['detection_boxes'])\n",
    "    detection_classes = [int(x) for x in detections['detection_classes']]\n",
    "    detection_scores = detections['detection_scores']\n",
    "    \n",
    "    # display results on image\n",
    "    image_np_with_detections = \\\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            img,\n",
    "            detection_boxes,\n",
    "            detection_classes,\n",
    "            detection_scores,\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=100,\n",
    "            min_score_thresh=0.6,\n",
    "            agnostic_mode=False)\n",
    "    images.append(image_np_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccfab78",
   "metadata": {},
   "source": [
    "We can verify that the model worked correctly by displaying elements of the `images` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b47b0a58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6971b68070>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a1ea0",
   "metadata": {},
   "source": [
    "Finally, we can create a video (`output.avi`) with our detections by running the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c31d836e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_width = images[0].shape[0]\n",
    "frame_height = images[0].shape[1]\n",
    "\n",
    "out = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "\n",
    "# Read and display the images\n",
    "for image in images:\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    out.write(image) # Write the image to the video\n",
    "    if cv2.waitKey(1) == ord('q'): # Hit `q` to exit\n",
    "        break\n",
    "        \n",
    "# Release everything if job is finished\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b288094",
   "metadata": {},
   "source": [
    "The video would be stored in the current working directory. You can download it from Sagemaker and run it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83a4bf-9d30-46ad-ad42-d8cbd89f9ed7",
   "metadata": {},
   "source": [
    "## Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46fb2108-a7f3-497b-94ca-0901b45875e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: tf2-object-detection-2024-05-28-08-13-10-336\n",
      "Endpoint Configuration Name: tf2-object-detection-2024-05-28-08-13-11-329\n",
      "Endpoint Name: tf2-object-detection-2024-05-28-08-13-11-329\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "endpoint_name = predictor.endpoint_name\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_config_name = response['EndpointConfigName']\n",
    "\n",
    "config_response = sagemaker.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "model_name = config_response['ProductionVariants'][0]['ModelName']\n",
    "\n",
    "print(\"Model Name:\", model_name)\n",
    "print(\"Endpoint Configuration Name:\", endpoint_config_name)\n",
    "print(\"Endpoint Name:\", endpoint_name)\n",
    "\n",
    "# Delete the endpoint\n",
    "sagemaker.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(\"Endpoint deleted.\")\n",
    "\n",
    "# Delete the endpoint configuration\n",
    "sagemaker.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "print(\"Endpoint configuration deleted.\")\n",
    "\n",
    "# Delete the model\n",
    "sagemaker.delete_model(ModelName=model_name)\n",
    "print(\"Model deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009c78a-aea8-46c5-9a28-3b303705e242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
